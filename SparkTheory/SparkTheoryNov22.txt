- Broadcast Hash Join (BHJ) VS. Sort Merge Join (SMJ)
    - https://youtu.be/B9aY7KkTLTw

    - Ignore executor side BHJ discussion as that was a suggested feature PR that got declined in the official spark repo:
    https://github.com/apache/spark/pull/15178

    - Q&A: On Linked In:
        - Tarek: 
        Hi Jianneng, I'm a novice big data engineer at Microsoft. I recently watched your presentation on youtube
        "On Improving Broadcast Joins in Apache Spark SQL". I really enjoyed it but would love your guidance on a 
        question in the youtube comments, could you please reply to Sarada Rout on the video?

        - Jianneng Li: 
        Hey Tarek,

        Thanks for reaching out after watching my talk, and sorry for the late reply.

        It's been a while since I worked on the broadcast join project, so I unfortunately no longer have enough context to post a thoughtful reply publicly. However, I'll try my best to give you some pointers in this message.

        Looking at my graph at 16:07 again, the numbers did look off: after the size of data was increased by 10x (60M to 600M), executor BHJ's time increased linearly (from under 1m to under 10m), while SMJ's time increased super-linearly (from under 1m to over 2h). If I had to guess, it was probably due to suboptimal partitioning of the input data, resulting in larger partitions than ideal to be read by each of the 18 cores. This would lead to memory contention/spilling etc, making I/O no longer the dominant component in runtime.

        To understand how Spark partitioning works, I highly recommend that you watch this video: https://www.youtube.com/watch?v=daXEp4HmS-E. It explains the concept of input vs shuffle partitions, as well as guidance on ideal partition size (100-200MB IIRC).

        Additionally, I'd encourage you to try and reproduce the numbers from the talk yourself. TPC-H is a well-known benchmark, and you can get the machine specs I used from the slides.

        Hope that helps,

        Jianneng

- Apache Spark Core—Deep Dive—Proper Optimization Daniel Tomes Databricks
    - https://www.youtube.com/watch?v=daXEp4HmS-E 

    - Undersanding baseline performance:
        - Check if spark actions are efficient: long stages, spills, laggard tasks...
        - Maximize CPU utilization on each action: +70% (Monitor on Ganglia, Yarn, etc.) --> Don't fully max out on production for safety.

    - Spark Partitions:
        - Input: 
            Spark mostly does a good job of auto sizing & splitting the input into partitions (as long as source data is splittable & of right format & compression type).
            Spark default size ~128 MB.
            Might want to decrease default if you explode rows in each partition OR increase if partitions < core count OR derease if using high memory footprint UDFs.
        - Shuffle: 
            200 default is wrong in majority of cases. 
            Optimal Partition Count ~= Stage Input Data Size / Target Size Per Partition
            Target Size should be 100-200 MB/partition ideally and can be tuned to minimize disk/memory spills.
            Partition count should be a factor of number of cores to maximize utilization.
        - Output: 
            Changed by repartition OR coalesce OR shuffle to desired partitions then write immediately after.
            Balances file size in output to make reads faster (more time to write but faster to read).
            Prefer Coalesce over Repartition as it's much faster, if you are reducing number of partitions.

    - @1:08:46 - Why do Scala UDFs often have poor performance?
        - No way to vectorize them as of May 2019.
        - Try your best to convert these to spark SQL functions.
        - They cost so much because you have to serialize & deserialize.
        - If you have to use these non-vectorize UDFs, only do that once for a spark action
        i.e. don't keep switching back & forth between spark SQL & java UDFs.
        i.e. group all those operations together as much as possible.

    - @1:13:14 - Usually there's a 5-10X improvement moving from RDDs to DataFrames.
    
    - @1:05:04 - How do you get the size in bytes of dataframe, in order to determine optimal shuffle partition count?
        - Counting number of partitions & multiplying by default input partition size (128 MB).
        - Estimate = number of rows * number of columns * average data type size per column.
        - Can be difficult & expensive in spark to find the size of transformed DF during runtime. 

    - Join Optimization: SortMergeJoin vs Broadcast Hash Join:
        - Use SMJ when both sides are large.
        - Use BHJ when one side is small.
        - BHJ can be dangerous on memory footprint & garbage collection, since it collects the entire data frame on driver & builds a bigger hashmap out of it.
        e.g. from video 130 MB DF turned into 270 MB broadcasted object --> speaker said this can be a dangerous size to broadcast (possibly due to spills?). 